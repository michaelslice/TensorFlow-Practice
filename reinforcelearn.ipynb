{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next and final topic in this course covers Reinforcement Learning. This technique is different than many of the other machine learning techniques we have seen earlier and has many applications in training agents (an AI) to interact with environment like games. Rather than feeding our machine learning model millions of examples we let our model come up with it's own examples by exploring an environment. The concept is fairly simple. Human learn by exploring and learning from mistakes and past experiences so let's have our computer do the same."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Terminology"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- Environment In reinforcemnt learning tasks we have a notion of the environment. This is what our agent will explore. An example of an environment in the case of training an AI to play say a game of mario would be the level we are training the agent on.\n",
    "\n",
    "-- Agent an agent is a nentity that is exploring the environment. Our agent will interact and take differernt actions within the environment. In our mario example the mario character within the game would be our agent.\n",
    "\n",
    "-- State at all times our agent will be in what we call a state. The state simply tells us about the status of the agent The most common example of a state is the location of the agent within the environment. Moving locations would change the agent state. \n",
    "\n",
    "-- Action any interaction between the agent and environment would be considered an action. For example moving to the left or jumping would be an action. An action may or may not change the current state of the agent. In fact the act of doing nothing is actually an action as well. The action of say not pressing a ket if we are using mario example.\n",
    "\n",
    "-- Reward every action that our agent takes will result in a reward of some magnitude (positive or negative). The goal of our agent will be to maximize it's reward in an environment. Sometimes the reward will be clear, for example if an agent performs an action which increases their score in the environement we could say they've recieved a positve reward. If the agent were to perform an action which results in them losing score or possibly dying in the environment then they would recieve a negative reward.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-Learning is a fairly simple yet quite powerful technique in machine learning that involves learning a matrix of action-reward values. This matrix is often reffered to as a Q-Table or Q-Matrix. The matrix is in shape (number of possible states, number of possible actions) where each value at matrix[n,m] represents the agents expected reward given they are in state n and take action m. The Q-learning algorithm defines the way we update the values in the matrix and decide what action to take at each state. The idea is that after a succusful training/learninh of this Q-Table/matrix we can determine the action an agent should take in any state by looking at that states row in the matrix and taking the maximum value column as the action."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning Example"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example we will use the Q-Learning algorithm to train an agent to navigate a popular environment from the OpenAI Gym. The Open AI Gym was developed so programmers could practice machine learning using unique environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading a environment\n",
    "env = gym.make('FrozenLake-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "print(env.observation_space.n) # get number of states\n",
    "print(env.action_space.n) # get number of actions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset() # result environment to default state "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = env.action_space.sample() # get random action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation, reward, done, info = env.step(action)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
